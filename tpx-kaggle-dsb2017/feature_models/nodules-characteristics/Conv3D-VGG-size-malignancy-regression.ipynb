{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lung nodule characterization <br/>\n",
    "Copyright (C) 2017 Therapixel (Pierre Fillard).\n",
    "\n",
    "Input data: LIDC (https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI) with\n",
    "nodule sizes (http://www.via.cornell.edu/lidc/). <br/>\n",
    "Beforehant, all series shall be converted to a volumetric format (ITK MHD). Filenames\n",
    "shall match the UID of each series. Binaries in the tools/ folder can be used for that \n",
    "(dataImporter, seriesExporter).\n",
    "\n",
    "Annotation files are provided in csv format. Those depict positions (in real world\n",
    "coordinates) from the series identified by the series UID (globally unique) where\n",
    "nodules can be found, along with their characterization (inc. size).\n",
    "\n",
    "This notebook will guide you through the process of training a deep net to classify\n",
    "nodules vs non-nodules. The following steps are involved:\n",
    "- data conversion: all annotations are turned into h5 arrays by extracting a patch\n",
    "of size 64x64x64 around each position. Images are all resampled to have the same\n",
    "voxel size of 0.625x0.625x0.625.\n",
    "- model training: 4xGPUs were used to train this model using data-parallelism. \n",
    "\n",
    "To improve detection accuracy, 2 models need to be trained with a different seed each\n",
    "time to split the dataset into training and validation. The final prediction probability\n",
    "of wether a patch is a nodule or not is the average of each individual prediction of\n",
    "each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import glob\n",
    "import time\n",
    "from time import sleep\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout, l2_regularizer\n",
    "from tensorflow.python.ops.nn import relu, elu, relu6, relu1, sigmoid, tanh, softmax\n",
    "from tensorflow.python.ops import variable_scope\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import lidc as lidc\n",
    "import TherapixelDL.image as tpxdli\n",
    "from six.moves import xrange\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(lidc)\n",
    "importlib.reload(tpxdli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readCSV(filename):\n",
    "    lines = []\n",
    "    with open(filename, 'r') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        for line in csvreader:\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nodule_csv_to_h5(nodule_file, data_directory, output_file):\n",
    "    nodules = readCSV(nodule_file)\n",
    "    nodules = nodules[1:] #skip header\n",
    "    nodule_count = len(nodules)\n",
    "    \n",
    "    patch_size = 64\n",
    "    patches = np.zeros(shape=(nodule_count,patch_size,patch_size,patch_size), dtype=np.float32)\n",
    "    nodule_type = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    num_reviews = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    is_large = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    subtlety = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    internalStructure = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    calcification = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    sphericity = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    margin = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    lobulation = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    spiculation = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    texture = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    malignancy = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    diameter = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    \n",
    "    target_spacing = [0.625,0.625,0.625]\n",
    "\n",
    "    pad_offset = patch_size//2\n",
    "\n",
    "    current_seriesuid = ''\n",
    "\n",
    "    for index in range(1,len(nodules)):\n",
    "        nodule = nodules[index]\n",
    "        seriesuid = nodule[0]\n",
    "\n",
    "        # find series\n",
    "        series_filename = data_directory + '/' + seriesuid + '.mhd'\n",
    "        if (not os.path.isfile(series_filename)):        \n",
    "            print('series not found:', seriesuid)\n",
    "            continue\n",
    "\n",
    "        if (seriesuid != current_seriesuid):\n",
    "            print('reading / resampling series:', seriesuid)\n",
    "            itk_image = sitk.ReadImage(series_filename)\n",
    "            volume_orig, origin, spacing, orientation = lidc.parse_itk_image(itk_image)\n",
    "\n",
    "            # resample using itk\n",
    "            padding_value = volume_orig.min()\n",
    "            img_z_orig, img_y_orig, img_x_orig = volume_orig.shape\n",
    "            img_z_new = int(np.round(img_z_orig*spacing[2]/target_spacing[2]))\n",
    "            img_y_new = int(np.round(img_y_orig*spacing[1]/target_spacing[1]))\n",
    "            img_x_new = int(np.round(img_x_orig*spacing[0]/target_spacing[0]))\n",
    "\n",
    "            itk_image_resampled = lidc.resample_itk_image(itk_image, [img_x_new,img_y_new,img_z_new], target_spacing, int(padding_value))\n",
    "            volume, _, _, _ = lidc.parse_itk_image(itk_image_resampled)\n",
    "            volume = volume.astype(np.float32)\n",
    "            volume = lidc.normalizePlanes(volume)\n",
    "            volume = np.pad(volume, ((pad_offset,pad_offset), (pad_offset,pad_offset), (pad_offset,pad_offset)), \n",
    "                            'constant', constant_values=((0, 0),(0, 0),(0, 0)))    \n",
    "            current_seriesuid = seriesuid\n",
    "            print('done')\n",
    "\n",
    "        x = float(nodule[1])\n",
    "        y = float(nodule[2])\n",
    "        z = float(nodule[3])\n",
    "\n",
    "        k,j,i = lidc.worldToVoxelCoord([x,y,z], origin, target_spacing, orientation)\n",
    "        k = int(round(k))\n",
    "        j = int(round(j))\n",
    "        i = int(round(i))\n",
    "        k = min(max(0, k), volume.shape[0]-patch_size)\n",
    "        j = min(max(0, j), volume.shape[1]-patch_size)\n",
    "        i = min(max(0, i), volume.shape[2]-patch_size)\n",
    "\n",
    "        patches[index-1] = volume[k:k+patch_size,j:j+patch_size,i:i+patch_size]\n",
    "        nodule_type[index-1] = int(nodule[4])\n",
    "        num_reviews[index-1] = int(nodule[5])\n",
    "        is_large[index-1] = int(nodule[6])    \n",
    "        subtlety[index-1] = float(nodule[7])\n",
    "        internalStructure[index-1] = int(nodule[8])\n",
    "        calcification[index-1] = int(nodule[9])\n",
    "        sphericity[index-1] = float(nodule[10])\n",
    "        margin[index-1] = float(nodule[11])\n",
    "        lobulation[index-1] = float(nodule[12])\n",
    "        spiculation[index-1] = float(nodule[13])\n",
    "        texture[index-1] = float(nodule[14])\n",
    "        malignancy[index-1] = float(nodule[15])\n",
    "        diameter[index-1] = float(nodule[16])\n",
    "\n",
    "    # create h5 dataset\n",
    "    h5_file = h5.File(output_file, 'w')\n",
    "    h5_file.create_dataset('PATCHES', data = patches, dtype=np.float32)\n",
    "    h5_file.create_dataset('LABELS', data = nodule_type, dtype=np.int32)\n",
    "    h5_file.create_dataset('REVIEWS', data = num_reviews, dtype=np.int32)\n",
    "    h5_file.create_dataset('IS_LARGE', data = is_large, dtype=np.int32)\n",
    "    h5_file.create_dataset('SUBTLETY', data = subtlety, dtype=np.float32)\n",
    "    h5_file.create_dataset('INTERNALSTRUCTURE', data = internalStructure, dtype=np.int32)\n",
    "    h5_file.create_dataset('CALCIFICATION', data = calcification, dtype=np.int32)\n",
    "    h5_file.create_dataset('SPHERICITY', data = sphericity, dtype=np.float32)\n",
    "    h5_file.create_dataset('MARGIN', data = margin, dtype=np.float32)\n",
    "    h5_file.create_dataset('LOBULATION', data = lobulation, dtype=np.float32)\n",
    "    h5_file.create_dataset('SPICULATION', data = spiculation, dtype=np.float32)\n",
    "    h5_file.create_dataset('TEXTURE', data = texture, dtype=np.float32)\n",
    "    h5_file.create_dataset('MALIGNANCY', data = malignancy, dtype=np.float32)\n",
    "    h5_file.create_dataset('DIAMETER', data = diameter, dtype=np.float32)\n",
    "    h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data conversion - once for all\n",
    "\n",
    "nodule_file = 'lidc_nodules_with_size.csv'\n",
    "data_directory = '/media/data/LIDC/LIDC-MHD/'\n",
    "output_file = 'lidc_nodules_with_size.h5'\n",
    "\n",
    "nodule_csv_to_h5(nodule_file, data_directory, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lidc_filepath = 'lidc_nodules_with_size.h5'\n",
    "\n",
    "train_data_list = []\n",
    "train_target_list = []\n",
    "validation_data_list = []\n",
    "validation_target_list = []\n",
    "\n",
    "val_ratio = 0.1\n",
    "\n",
    "REVIEWS_MIN=1.\n",
    "REVIEWS_MAX = 4.\n",
    "SUBTLETY_MIN = 1.\n",
    "SUBTLETY_MAX = 5.\n",
    "SPHERICITY_MIN = 1.\n",
    "SPHERICITY_MAX = 5.\n",
    "MARGIN_MIN = 1.\n",
    "MARGIN_MAX = 5.\n",
    "LOBULATION_MIN = 1.\n",
    "LOBULATION_MAX = 5.\n",
    "SPICULATION_MIN = 1.\n",
    "SPICULATION_MAX = 5.\n",
    "TEXTURE_MIN = 1.\n",
    "TEXTURE_MAX = 5.\n",
    "MALIGNANCY_MIN = 1.\n",
    "MALIGNANCY_MAX = 5.\n",
    "DIAMETER_MIN = 0.\n",
    "DIAMETER_MAX = 20.\n",
    "\n",
    "# read nodules\n",
    "h5_file = h5.File(lidc_filepath, 'r')\n",
    "patches = h5_file['PATCHES'][...]\n",
    "labels = h5_file['LABELS'][...]\n",
    "is_large = h5_file['IS_LARGE'][...]\n",
    "malignancy = h5_file['MALIGNANCY'][...]\n",
    "diameter = h5_file['DIAMETER'][...]\n",
    "h5_file.close()\n",
    "\n",
    "malignancy = (malignancy.astype(np.float32)-MALIGNANCY_MIN)/(MALIGNANCY_MAX-MALIGNANCY_MIN)\n",
    "diameter = (diameter.astype(np.float32)-DIAMETER_MIN)/(DIAMETER_MAX-DIAMETER_MIN)\n",
    "\n",
    "indices = np.arange(patches.shape[0])\n",
    "indices = indices[(labels==1) & (is_large==1) & (diameter>0.)] # only large nodules with size\n",
    "\n",
    "# concat\n",
    "malignancy = malignancy.reshape(tuple(malignancy.shape)+tuple([1]))\n",
    "diameter = diameter.reshape(tuple(diameter.shape)+tuple([1]))\n",
    "\n",
    "targets = np.concatenate((malignancy,diameter), axis=1)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "count = len(indices)\n",
    "val_count = int(count * val_ratio)\n",
    "train_count = count - val_count\n",
    "train_patches = patches[indices[val_count:]]\n",
    "train_targets = targets[indices[val_count:]]\n",
    "val_patches = patches[indices[:val_count]]\n",
    "val_targets = targets[indices[:val_count]]\n",
    "    \n",
    "train_data_list.append(train_patches)\n",
    "train_target_list.append(train_targets)\n",
    "validation_data_list.append(val_patches)\n",
    "validation_target_list.append(val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('is_large', is_large[indices].min(), is_large[indices].max())\n",
    "print('malignancy', malignancy[indices].min(), malignancy[indices].max())\n",
    "print('diameter', diameter[indices].min(), diameter[indices].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperameters of the model\n",
    "channels = 1\n",
    "scalings=None #np.array([1.0, 1.5])\n",
    "offsets=None #np.array([0.0, -0.05])\n",
    "depth = 64\n",
    "height = 64\n",
    "width = 64\n",
    "num_gpus = 4\n",
    "batch_size = 32\n",
    "patch_size = 64\n",
    "gpu_mem_ratio = 1.0\n",
    "num_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shift_range = 0.0\n",
    "image_gen_3d = tpxdli.ImageDataGenerator3D(rotation_range=180.0, width_shift_range=shift_range, height_shift_range=shift_range, depth_shift_range=shift_range,\n",
    "                                           shear_range=0.1, zoom_range=np.array([0.95,1.05], dtype=np.float32),\n",
    "                                           horizontal_flip=True, vertical_flip=True, depth_flip=True,\n",
    "                                           windowing_scale_range=0.0, windowing_intercept_range=0.0,\n",
    "                                           dim_ordering = 'tf')\n",
    "# do not augment validation batch to simulate real-life data\n",
    "image_gen_3d_val = tpxdli.ImageDataGenerator3D(rotation_range=0.0, width_shift_range=0.0, height_shift_range=0.0, depth_shift_range=0.0,\n",
    "                                           shear_range=0.0, zoom_range=np.array([1.0,1.0], dtype=np.float32), horizontal_flip=False, vertical_flip=False, depth_flip=False,\n",
    "                                           dim_ordering = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(train_data, train_targets, validation_data, validation_targets, lr_scheme, num_gpus=1, num_epochs=100,\n",
    "          output_dir='', prev_model=''):\n",
    "        \n",
    "    # reset graph first\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "        is_training = tf.placeholder(tf.bool, shape=[], name='is_training')\n",
    "    \n",
    "        # Setting up placeholder, this is where your data enters the graph!\n",
    "        x_pl = tf.placeholder(tf.float32, shape=(None, height, width, depth, channels), name='data_x')\n",
    "        y_pl = tf.placeholder(tf.float32, shape=(None, num_outputs), name='data_y')\n",
    "    \n",
    "        # defining our optimizer\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []    \n",
    "        losses = []\n",
    "        y = []\n",
    "    \n",
    "        x_splits = tf.split(x_pl, num_or_size_splits=num_gpus)\n",
    "        y_splits = tf.split(y_pl, num_or_size_splits=num_gpus)\n",
    "    \n",
    "        with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "            for i in range(num_gpus):\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('tower_%d' % (i)) as scope:\n",
    "\n",
    "                        preds, _ = lidc.inference_all_scores(x_splits[i], \n",
    "                                                          num_outputs=num_outputs,\n",
    "                                                          is_training=is_training)\n",
    "                        preds = tf.squeeze(preds)\n",
    "                        l = lidc.l2_loss(preds=preds, values=y_splits[i], with_regularization=False)\n",
    "\n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                        # Calculate the gradients for the batch of data\n",
    "                        grads = optimizer.compute_gradients(l)\n",
    "\n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "                        losses.append(l)\n",
    "                        y.append(preds)\n",
    "\n",
    "        # We must calculate the mean of each gradient. Note that this is the\n",
    "        # synchronization point across all towers.\n",
    "        if (num_gpus>1):\n",
    "            grads = lidc.average_gradients(tower_grads)    \n",
    "        else:\n",
    "            grads = tower_grads[0]\n",
    "    \n",
    "        # Apply the gradients to adjust the shared variables.\n",
    "        apply_gradient_op = optimizer.apply_gradients(grads)\n",
    "    \n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(lidc.MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "            \n",
    "        # Restore the moving average version of the learned variables for eval.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(lidc.MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore, max_to_keep=None)\n",
    "        \n",
    "        # restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "        gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_mem_ratio)\n",
    "        # initialize the Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts, \n",
    "                                                allow_soft_placement=True, \n",
    "                                                log_device_placement=True)) # allow_soft_placement=True needed to make batch_normalization work accross GPU\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if (prev_model):\n",
    "            print('restoring model', prev_model)\n",
    "            saver.restore(sess, prev_model)            \n",
    "    \n",
    "    n_train_samples = train_count\n",
    "    train_capacity = batch_size * num_gpus\n",
    "    num_batches_train = n_train_samples // train_capacity\n",
    "    \n",
    "    train_iterator_3d = image_gen_3d.flowList(X=train_data, \n",
    "                                              Y=train_targets, \n",
    "                                              batch_size=train_capacity,\n",
    "                                              balance=False,\n",
    "                                              shuffle=True, \n",
    "                                              output_depth=patch_size, \n",
    "                                              output_rows=patch_size, \n",
    "                                              output_cols=patch_size,\n",
    "                                              num_output_channels=channels,\n",
    "                                              scalings=scalings,\n",
    "                                              offsets=offsets)\n",
    "    \n",
    "    val_capacity = batch_size * num_gpus\n",
    "    val_iterator_3d = image_gen_3d_val.flowList(X=validation_data, \n",
    "                                                Y=validation_targets,\n",
    "                                                batch_size=val_capacity,\n",
    "                                                balance=False,\n",
    "                                                shuffle=False,\n",
    "                                                output_depth=patch_size, \n",
    "                                                output_rows=patch_size, \n",
    "                                                output_cols=patch_size,\n",
    "                                                num_output_channels=channels,\n",
    "                                                scalings=scalings,\n",
    "                                                offsets=offsets)\n",
    "    \n",
    "    n_val_samples = val_count\n",
    "    num_batches_valid = n_val_samples // val_capacity    \n",
    "\n",
    "    print('training with parameters:\\n\\t- train capacity: %d\\n\\t- val capacity: %d\\n\\t- batch size: %d\\n\\t- patch size: %d\\n\\t'\\\n",
    "          '- num gpu: %d\\n\\t- num epochs: %d\\n\\t- previous model: %s' % (n_train_samples, n_val_samples, batch_size, patch_size,\n",
    "                                                                         num_gpus, num_epochs, prev_model))           \n",
    "    \n",
    "    print('number of training batches per epoch', num_batches_train)\n",
    "    print('number of validation batches per epoch', num_batches_valid)\n",
    "    \n",
    "    lr = -1\n",
    "    best_val_loss = -1.\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "    \n",
    "    train_queue = tpxdli.QueuedIterator(train_iterator_3d, num_batches_train)\n",
    "    val_queue = tpxdli.QueuedIterator(val_iterator_3d, num_batches_valid)\n",
    "    \n",
    "    try:\n",
    "        # init best_val_loss before training\n",
    "        val_queue.produce()\n",
    "        for i in range(num_batches_valid):\n",
    "            (batch_val_x, batch_val_y) = val_queue.get_queue().get()            \n",
    "            feed_dict_eval = {\n",
    "                x_pl: batch_val_x,\n",
    "                y_pl: batch_val_y,\n",
    "                is_training: False\n",
    "            }\n",
    "            fetches_eval = [losses]\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_eval, feed_dict=feed_dict_eval)\n",
    "            # collecting and storing predictions\n",
    "            cur_loss = np.sum(res[0])\n",
    "            if i==0:\n",
    "                valid_loss = cur_loss/(num_gpus)\n",
    "            else:\n",
    "                valid_loss = valid_loss*i/(i+1) + cur_loss/(num_gpus*(i+1))\n",
    "            val_queue.get_queue().task_done()\n",
    "            sys.stdout.write('\\rValidation. batch: %d/%d. loss: %f'%(i+1,num_batches_valid,valid_loss))\n",
    "            sys.stdout.flush()\n",
    "            sleep(1)\n",
    "            \n",
    "        best_val_loss = valid_loss\n",
    "        print('\\nInitial validation loss is: %f'%(best_val_loss))                \n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            if (lr != lr_scheme[epoch]):\n",
    "                lr = lr_scheme[epoch]\n",
    "                print('using lr', lr)\n",
    "        \n",
    "            t0 = time.time()                \n",
    "                        \n",
    "            train_queue.produce()\n",
    "            for i in range(num_batches_train):                \n",
    "                (batch_train_x, batch_train_y) = train_queue.get_queue().get()                \n",
    "                feed_dict_train = {\n",
    "                    x_pl: batch_train_x,\n",
    "                    y_pl: batch_train_y,\n",
    "                    is_training: True, \n",
    "                    learning_rate: lr\n",
    "                }\n",
    "                fetches_train = [train_op, losses]\n",
    "                res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "                cur_loss = np.sum(res[1])\n",
    "                if i==0:\n",
    "                    train_loss = cur_loss/(num_gpus)\n",
    "                else:\n",
    "                    train_loss = train_loss*i/(i+1) + cur_loss/(num_gpus*(i+1))                                \n",
    "                train_queue.get_queue().task_done()\n",
    "                sys.stdout.write('\\rTraining. batch: %d/%d, loss: %f'%(i+1,num_batches_train,train_loss))\n",
    "                sys.stdout.flush()\n",
    "                sleep(1)\n",
    "                    \n",
    "            t1 = time.time()\n",
    "            epoch_time = t1 - t0\n",
    "        \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        \n",
    "            val_queue.produce()\n",
    "            #p_preds = []\n",
    "            #p_true = []\n",
    "            for i in range(num_batches_valid):\n",
    "                (batch_val_x, batch_val_y) = val_queue.get_queue().get()                \n",
    "                feed_dict_eval = {\n",
    "                    x_pl: batch_val_x,\n",
    "                    y_pl: batch_val_y,\n",
    "                    is_training: False\n",
    "                }\n",
    "                fetches_eval = [losses, preds]\n",
    "                # running the validation\n",
    "                res = sess.run(fetches=fetches_eval, feed_dict=feed_dict_eval)\n",
    "                # collecting and storing predictions\n",
    "                cur_loss = np.sum(res[0])\n",
    "                if i==0:\n",
    "                    valid_loss = cur_loss/num_gpus\n",
    "                else:\n",
    "                    valid_loss = valid_loss*i/(i+1) + cur_loss/(num_gpus*(i+1))\n",
    "                #p_preds.append(res[1])\n",
    "                #p_true.append(batch_val_y)\n",
    "                val_queue.get_queue().task_done()                \n",
    "                sys.stdout.write('\\rValidation. batch: %d/%d, loss: %f'%(i+1,num_batches_valid,valid_loss))\n",
    "                sys.stdout.flush()\n",
    "                sleep(1)\n",
    "                            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "            \n",
    "            #print('preds', np.concatenate(p_preds))\n",
    "            #print('truth', np.concatenate(p_true))\n",
    "\n",
    "            print (\"Epoch %i: train loss %e, valid loss %f, epoch time %.2f s \" \\\n",
    "            % (epoch+1, train_loss, valid_loss, epoch_time))\n",
    "        \n",
    "            if (best_val_loss<0):\n",
    "                best_val_loss = valid_loss\n",
    "            \n",
    "            if ((best_val_loss>=0) and (valid_loss<best_val_loss)):\n",
    "                print('val loss improved from %f to %f, saving model' % (best_val_loss, valid_loss))\n",
    "                best_val_loss = valid_loss\n",
    "                if (output_dir):\n",
    "                    filename = output_dir + 'best_model_loss'\n",
    "                    print('saving model to file:',filename)\n",
    "                    saver.save(sess, filename)\n",
    "                    \n",
    "            if ((epoch%10)==0):\n",
    "                saver.save(sess, output_dir+'checkpoint-epoch-%.3d-loss-%.4f'%(epoch,valid_loss))\n",
    "                \n",
    "            epoch += 1\n",
    "            \n",
    "            train_loss = 0.\n",
    "            valid_loss = 0.\n",
    "            \n",
    "    except KeyboardInterrupt:        \n",
    "        pass\n",
    "    \n",
    "    train_queue.get_queue().join()\n",
    "    val_queue.get_queue().join()\n",
    "    \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "prev_model = ''\n",
    "\n",
    "# to run:\n",
    "# - once 300 epochs with AdamOptimizer and lr=1e-3 then 1e-4 after 100 epochs\n",
    "# - once 300 epochs with SGB and lr=1e-5\n",
    "# Learn 2 models as above and average predictions\n",
    "\n",
    "output_directory = 'model_1/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "lr_scheme = np.zeros(shape=(num_epochs), dtype=np.float32)\n",
    "lr = 1e-3 #1e-4\n",
    "lr_decay = 10.\n",
    "lr_scheme[0:100] = lr\n",
    "lr /= lr_decay\n",
    "lr_scheme[100:] = lr\n",
    "    \n",
    "# train model    \n",
    "train(train_data=train_data_list, \n",
    "      train_targets=train_target_list,\n",
    "      validation_data=validation_data_list, \n",
    "      validation_targets=validation_target_list,\n",
    "      lr_scheme=lr_scheme,\n",
    "      num_gpus=num_gpus,\n",
    "      num_epochs=num_epochs,      \n",
    "      output_dir=output_directory,\n",
    "      prev_model=prev_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
