{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lung nodule classification <br/>\n",
    "Copyright (C) 2017 Therapixel (Pierre Fillard).\n",
    "\n",
    "Input data: LIDC (https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI). <br/>\n",
    "Beforehant, all series shall be converted to a volumetric format (ITK MHD). Filenames\n",
    "shall match the UID of each series. Binaries in the tools/ folder can be used for that \n",
    "(dataImporter, seriesExporter).\n",
    "\n",
    "Annotation files are provided in csv format. Those depict positions (in real world\n",
    "coordinates) from the series identified by the series UID (globally unique) where\n",
    "nodules can be found, along with their characterization (number of positive reviews,\n",
    "malignancy, etc.). Negatives (non-nodules) are also provided.\n",
    "\n",
    "This notebook will guide you through the process of training a deep net to classify\n",
    "nodules vs non-nodules. The following steps are involved:\n",
    "- data conversion: all annotations are turned into h5 arrays by extracting a patch\n",
    "of size 64x64x64 around each position. Images are all resampled to have the same\n",
    "voxel size of 0.625x0.625x0.625.\n",
    "- model training: 2xGPUs were used to train this model using data-parallelism. A\n",
    "patched version of tensorflow is needed (it is provided in the tools/ directory for\n",
    "convenience, but the code can be found here:\n",
    "https://github.com/pfillard/tensorflow/tree/r1.0_relu1)\n",
    "This version has an extra activation function (relu1) which attempts at learning the\n",
    "right windowing parameters to best view nodules.\n",
    "\n",
    "To improve detection accuracy, 5 models need to be trained with a different seed each\n",
    "time to split the dataset into training and validation. The final prediction probability\n",
    "of wether a patch is a nodule or not is the average of each individual prediction of\n",
    "each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import glob\n",
    "import time\n",
    "from time import sleep\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected, convolution2d, flatten, batch_norm, max_pool2d, dropout, l2_regularizer\n",
    "from tensorflow.python.ops.nn import relu, elu, relu6, relu1, sigmoid, tanh, softmax\n",
    "from tensorflow.python.ops import variable_scope\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import lidc as lidc\n",
    "import TherapixelDL.image as tpxdli\n",
    "from six.moves import xrange\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "import csv\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(lidc)\n",
    "importlib.reload(tpxdli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readCSV(filename):\n",
    "    lines = []\n",
    "    with open(filename, 'r') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        for line in csvreader:\n",
    "            lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert a csv nodule file into a h5 array. Requires series to be converted in mhd\n",
    "def nodules_to_h5(nodule_file, data_directory, output_file):\n",
    "    patch_size = 64\n",
    "    \n",
    "    nodules = readCSV(nodule_file)\n",
    "    nodules = nodules[1:] # skip header\n",
    "    nodule_count = len(nodules)\n",
    "    \n",
    "    # allocate arrays\n",
    "    patches = np.zeros(shape=(nodule_count,patch_size,patch_size,patch_size), dtype=np.float32)\n",
    "    nodule_type = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    num_reviews = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    is_large = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    subtlety = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    internalStructure = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    calcification = np.zeros(shape=(nodule_count), dtype=np.int32)\n",
    "    sphericity = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    margin = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    lobulation = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    spiculation = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    texture = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    malignancy = np.zeros(shape=(nodule_count), dtype=np.float32)\n",
    "    \n",
    "    target_spacing = [0.625,0.625,0.625]\n",
    "\n",
    "    pad_offset = patch_size//2\n",
    "\n",
    "    current_seriesuid = ''\n",
    "\n",
    "    for index in range(nodule_count):\n",
    "        nodule = nodules[index]\n",
    "        seriesuid = nodule[0]\n",
    "    \n",
    "        # find series\n",
    "        series_filename = data_directory + '/' + seriesuid + '.mhd'\n",
    "        \n",
    "        if (not os.path.isfile(series_filename)):        \n",
    "            print('series not found:', seriesuid)\n",
    "            continue\n",
    "    \n",
    "        if (seriesuid != current_seriesuid):\n",
    "            print('reading / resampling series:', seriesuid)\n",
    "            itk_image = sitk.ReadImage(series_filename)\n",
    "            volume_orig, origin, spacing, orientation = lidc.parse_itk_image(itk_image)\n",
    "    \n",
    "            # resample using itk - more accurate\n",
    "            padding_value = volume_orig.min()\n",
    "            img_z_orig, img_y_orig, img_x_orig = volume_orig.shape\n",
    "            img_z_new = int(np.round(img_z_orig*spacing[2]/target_spacing[2]))\n",
    "            img_y_new = int(np.round(img_y_orig*spacing[1]/target_spacing[1]))\n",
    "            img_x_new = int(np.round(img_x_orig*spacing[0]/target_spacing[0]))\n",
    "\n",
    "            itk_image_resampled = lidc.resample_itk_image(itk_image, [img_x_new,img_y_new,img_z_new], \n",
    "                                                          target_spacing, int(padding_value))\n",
    "            volume, _, _, _ = lidc.parse_itk_image(itk_image_resampled)\n",
    "            volume = volume.astype(np.float32)\n",
    "            volume = lidc.normalizePlanes(volume)\n",
    "            volume = np.pad(volume, ((pad_offset,pad_offset), (pad_offset,pad_offset), (pad_offset,pad_offset)), \n",
    "                            'constant', constant_values=((0, 0),(0, 0),(0, 0)))    \n",
    "            current_seriesuid = seriesuid\n",
    "            print('done')\n",
    "    \n",
    "        x = float(nodule[1])\n",
    "        y = float(nodule[2])\n",
    "        z = float(nodule[3])\n",
    "    \n",
    "        k,j,i = lidc.worldToVoxelCoord([x,y,z], origin, target_spacing, orientation)\n",
    "        k = int(round(k))\n",
    "        j = int(round(j))\n",
    "        i = int(round(i))\n",
    "        k = min(max(0, k), volume.shape[0]-patch_size)\n",
    "        j = min(max(0, j), volume.shape[1]-patch_size)\n",
    "        i = min(max(0, i), volume.shape[2]-patch_size)\n",
    "    \n",
    "        patches[index-1] = volume[k:k+patch_size,j:j+patch_size,i:i+patch_size]\n",
    "        nodule_type[index-1] = int(nodule[4])\n",
    "        num_reviews[index-1] = int(nodule[5])\n",
    "        is_large[index-1] = int(nodule[6])    \n",
    "        subtlety[index-1] = float(nodule[7])\n",
    "        internalStructure[index-1] = int(nodule[8])\n",
    "        calcification[index-1] = int(nodule[9])\n",
    "        sphericity[index-1] = float(nodule[10])\n",
    "        margin[index-1] = float(nodule[11])\n",
    "        lobulation[index-1] = float(nodule[12])\n",
    "        spiculation[index-1] = float(nodule[13])\n",
    "        texture[index-1] = float(nodule[14])\n",
    "        malignancy[index-1] = float(nodule[15])\n",
    "\n",
    "    # create h5 dataset\n",
    "    h5_file = h5.File(output_file, 'w')\n",
    "    h5_file.create_dataset('PATCHES', data = patches, dtype=np.float32)\n",
    "    h5_file.create_dataset('LABELS', data = nodule_type, dtype=np.int32)\n",
    "    h5_file.create_dataset('REVIEWS', data = num_reviews, dtype=np.int32)\n",
    "    h5_file.create_dataset('IS_LARGE', data = is_large, dtype=np.int32)\n",
    "    h5_file.create_dataset('SUBTLETY', data = subtlety, dtype=np.float32)\n",
    "    h5_file.create_dataset('INTERNALSTRUCTURE', data = internalStructure, dtype=np.int32)\n",
    "    h5_file.create_dataset('CALCIFICATION', data = calcification, dtype=np.int32)\n",
    "    h5_file.create_dataset('SPHERICITY', data = sphericity, dtype=np.float32)\n",
    "    h5_file.create_dataset('MARGIN', data = margin, dtype=np.float32)\n",
    "    h5_file.create_dataset('LOBULATION', data = lobulation, dtype=np.float32)\n",
    "    h5_file.create_dataset('SPICULATION', data = spiculation, dtype=np.float32)\n",
    "    h5_file.create_dataset('TEXTURE', data = texture, dtype=np.float32)\n",
    "    h5_file.create_dataset('MALIGNANCY', data = malignancy, dtype=np.float32)\n",
    "    h5_file.create_dataset('DIAMETER', data = diameter, dtype=np.float32)\n",
    "    h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data conversion - to run once\n",
    "nodule_file = 'lidc_nodules.csv'\n",
    "extra_nodule_file = 'lidc_extra_nodules.csv'\n",
    "false_positive_file = 'lidc_false_positives.csv'\n",
    "\n",
    "data_directory = '/media/data/LIDC/LIDC-MHD/' # shall contain all LIDC in mhd format, and named by their seriesuid\n",
    "\n",
    "nodules_to_h5(nodule_file, data_directory, 'lidc_nodules.h5')\n",
    "nodules_to_h5(extra_nodule_file, data_directory, 'lidc_extra_nodules.h5')\n",
    "nodules_to_h5(false_positive_file, data_directory, 'lidc_false_positives.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read data and split in training vs validation set\n",
    "\n",
    "nodule_files = ['lidc_nodules.h5', 'lidc_extra_nodules.h5']\n",
    "negative_files = ['lidc_false_positives.h5']\n",
    "\n",
    "num_reviews_min = 3 # use only nodule with at least 3/4 positive reviews\n",
    "\n",
    "val_ratio = 0.1 # validation ratio (10% of dataset)\n",
    "\n",
    "train_data = []\n",
    "train_targets = []\n",
    "validation_data = []\n",
    "validation_targets = []\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_positive_count = 0\n",
    "train_negative_count = 0\n",
    "val_positive_count = 0\n",
    "val_negative_count = 0\n",
    "\n",
    "for i in range(len(nodule_files)):\n",
    "    h5_file = h5.File(nodule_files[i], 'r')\n",
    "    patches = h5_file['PATCHES'] #don't read on purpose    \n",
    "    labels = h5_file['LABELS'][...]\n",
    "    is_large = h5_file['IS_LARGE'][...]\n",
    "    reviews = h5_file['REVIEWS'][...]\n",
    "    \n",
    "    count = patches.shape[0]\n",
    "    indices = np.arange(count)    \n",
    "    # special case of positives: keep only large nodules with at least 3 reviews\n",
    "    indices = indices[(labels==1) & (is_large==1) & (reviews>=num_reviews_min)]\n",
    "    \n",
    "    np.random.shuffle(indices)\n",
    "    count = len(indices)\n",
    "    val_count = int(count * val_ratio)\n",
    "    \n",
    "    train_data.append(patches[indices[val_count:]])\n",
    "    train_targets.append(labels[indices[val_count:]])\n",
    "    validation_data.append(patches[indices[:val_count]])\n",
    "    validation_targets.append(labels[indices[:val_count]])\n",
    "    \n",
    "    train_positive_count += (targets[indices[val_count:]]==1).sum()\n",
    "    train_negative_count += (targets[indices[val_count:]]==0).sum()\n",
    "    val_positive_count += (targets[indices[:val_count]]==1).sum()\n",
    "    val_negative_count += (targets[indices[:val_count]]==0).sum()\n",
    "    \n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "for i in range(len(negative_files)):\n",
    "    h5_file = h5.File(negative_files[i], 'r')\n",
    "    patches = h5_file['PATCHES'][...]    \n",
    "    labels = h5_file['LABELS'][...]\n",
    "\n",
    "    count = patches.shape[0]\n",
    "    val_count = int(count * val_ratio)\n",
    "    \n",
    "    indices = np.arange(count)    \n",
    "    np.random.shuffle(indices)\n",
    "        \n",
    "    train_data.append(patches[indices[val_count:]])\n",
    "    train_targets.append(labels[indices[val_count:]])\n",
    "    validation_data.append(patches[indices[:val_count]])\n",
    "    validation_targets.append(labels[indices[:val_count]])\n",
    "    \n",
    "    train_positive_count += (targets[indices[val_count:]]==1).sum()\n",
    "    train_negative_count += (targets[indices[val_count:]]==0).sum()\n",
    "    val_positive_count += (targets[indices[:val_count]]==1).sum()\n",
    "    val_negative_count += (targets[indices[:val_count]]==0).sum()\n",
    "    \n",
    "    h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_positive_count, train_negative_count, val_positive_count, val_negative_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# hyperameters of the model\n",
    "channels = 1\n",
    "scalings=None\n",
    "offsets=None\n",
    "depth = 48\n",
    "height = 48\n",
    "width = 48\n",
    "num_gpus = 2\n",
    "batch_size = 40\n",
    "patch_size = 48\n",
    "gpu_mem_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shift_range = 0.05\n",
    "image_gen_3d = tpxdli.ImageDataGenerator3D(rotation_range=180.0, width_shift_range=shift_range, height_shift_range=shift_range, depth_shift_range=shift_range,\n",
    "                                           shear_range=0.0, zoom_range=np.array([0.95,1.05], dtype=np.float32), horizontal_flip=True, vertical_flip=True, depth_flip=True,\n",
    "                                           windowing_scale_range=0.0, windowing_intercept_range=0.0,\n",
    "                                           dim_ordering = 'tf')\n",
    "# do not augment validation batch to simulate real-life data\n",
    "image_gen_3d_val = tpxdli.ImageDataGenerator3D(rotation_range=0.0, width_shift_range=0.0, height_shift_range=0.0, depth_shift_range=0.0,\n",
    "                                           shear_range=0.0, zoom_range=np.array([1.0,1.0], dtype=np.float32), horizontal_flip=False, vertical_flip=False, depth_flip=False,\n",
    "                                           dim_ordering = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(train_data, train_targets, validation_data, validation_targets, num_gpus=1, with_bn=False, \n",
    "          output_dir='', prev_model=''):\n",
    "    from TherapixelDL.confusionmatrix import ConfusionMatrix\n",
    "        \n",
    "    # reset graph first\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "        is_training = tf.placeholder(tf.bool, shape=[], name='is_training')\n",
    "    \n",
    "        # Setting up placeholder, this is where your data enters the graph!\n",
    "        x_pl = tf.placeholder(tf.float32, shape=(None, height, width, depth, channels), name='data_x')\n",
    "        y_pl = tf.placeholder(tf.int32, shape=(None, ), name='data_y')\n",
    "    \n",
    "        # defining our optimizer\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []    \n",
    "        losses = []\n",
    "        y = []\n",
    "    \n",
    "        x_splits = tf.split(x_pl, num_or_size_splits=num_gpus)\n",
    "        y_splits = tf.split(y_pl, num_or_size_splits=num_gpus)\n",
    "    \n",
    "        with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "            for i in range(num_gpus):\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('tower_%d' % (i)) as scope:\n",
    "                                        \n",
    "                        logits, _ = lidc.inference(x_splits[i], is_training=is_training, with_bn=with_bn)\n",
    "                        l = lidc.loss(logits, y_splits[i], with_regularization=False)\n",
    "                    \n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "                        # Calculate the gradients for the batch of data\n",
    "                        grads = optimizer.compute_gradients(l)\n",
    "            \n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "                        losses.append(l)\n",
    "                        y.append(tf.nn.softmax(logits))\n",
    "\n",
    "        # We must calculate the mean of each gradient. Note that this is the\n",
    "        # synchronization point across all towers.\n",
    "        if (num_gpus>1):\n",
    "            grads = lidc.average_gradients(tower_grads)    \n",
    "        else:\n",
    "            grads = tower_grads[0]\n",
    "    \n",
    "        # Apply the gradients to adjust the shared variables.\n",
    "        apply_gradient_op = optimizer.apply_gradients(grads)\n",
    "    \n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(lidc.MOVING_AVERAGE_DECAY, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "            \n",
    "        # Restore the moving average version of the learned variables for eval.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(lidc.MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore, max_to_keep=None)\n",
    "        \n",
    "        # restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "        gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_mem_ratio)\n",
    "        # initialize the Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts, \n",
    "                                                allow_soft_placement=True, \n",
    "                                                log_device_placement=True)) # allow_soft_placement=True needed to make batch_normalization work accross GPU\n",
    "                \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if (prev_model):\n",
    "            print('restoring model', prev_model)\n",
    "            saver.restore(sess, prev_model)\n",
    "    \n",
    "    n_train_samples = train_negative_count * 2        \n",
    "    train_capacity = batch_size * num_gpus\n",
    "    num_batches_train = n_train_samples // train_capacity\n",
    "    \n",
    "    train_iterator_3d = image_gen_3d.flowList(X=train_data, \n",
    "                                              Y=train_targets, \n",
    "                                              batch_size=train_capacity,\n",
    "                                              balance=True,\n",
    "                                              shuffle=True, \n",
    "                                              output_depth=patch_size, \n",
    "                                              output_rows=patch_size, \n",
    "                                              output_cols=patch_size,\n",
    "                                              num_output_channels=channels,\n",
    "                                              scalings=scalings,\n",
    "                                              offsets=offsets)\n",
    "    \n",
    "    val_capacity = batch_size * num_gpus\n",
    "    val_iterator_3d = image_gen_3d_val.flowList(X=validation_data, \n",
    "                                                Y=validation_targets,\n",
    "                                                batch_size=val_capacity,\n",
    "                                                balance=True, # to avoid negatives to overcome loss\n",
    "                                                shuffle=True, # to randomize validation batch\n",
    "                                                output_depth=patch_size, \n",
    "                                                output_rows=patch_size, \n",
    "                                                output_cols=patch_size,\n",
    "                                                num_output_channels=channels,\n",
    "                                                scalings=scalings,\n",
    "                                                offsets=offsets)\n",
    "    \n",
    "    n_val_samples = val_negative_count * 2 # val_iterator_3d.count()\n",
    "    num_batches_valid = n_val_samples // val_capacity    \n",
    "\n",
    "    print('training with parameters:\\n\\t- train capacity: %d\\n\\t- val capacity: %d\\n\\t- batch size: %d\\n\\t- patch size: %d\\n\\t'\\\n",
    "          '- num gpu: %d\\n\\t- num epochs: %d\\n\\t- previous model: %s' % (n_train_samples, n_val_samples, batch_size, patch_size,\n",
    "                                                                         num_gpus, num_epochs, prev_model))           \n",
    "    \n",
    "    print('number of training batches per epoch', num_batches_train)\n",
    "    print('number of validation batches per epoch', num_batches_valid)\n",
    "\n",
    "    train_acc, train_loss = [], []\n",
    "    valid_acc, valid_loss = [], []\n",
    "    test_acc, test_loss = [], []\n",
    "    lr_scheme = np.zeros(shape=(num_epochs), dtype=np.float32)\n",
    "    lr_scheme[:] = 0.0001\n",
    "    lr = -1\n",
    "    best_val_loss = -1.\n",
    "    best_val_acc = 0.\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "    \n",
    "    train_queue = tpxdli.QueuedIterator(train_iterator_3d, num_batches_train)\n",
    "    val_queue = tpxdli.QueuedIterator(val_iterator_3d, num_batches_valid)\n",
    "    \n",
    "    try:\n",
    "        # init best_val_loss before training\n",
    "        confusion_valid = ConfusionMatrix(2)            \n",
    "        val_queue.produce()\n",
    "        for i in range(num_batches_valid):\n",
    "            (batch_val_x, batch_val_y) = val_queue.get_queue().get()            \n",
    "            feed_dict_eval = {\n",
    "                x_pl: batch_val_x,\n",
    "                y_pl: batch_val_y,\n",
    "                is_training: False\n",
    "            }\n",
    "            fetches_eval = [y, losses]\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_eval, feed_dict=feed_dict_eval)\n",
    "            # collecting and storing predictions\n",
    "            cur_loss = np.sum(res[1])\n",
    "            preds = np.argmax(np.concatenate(res[0]), axis=-1)             \n",
    "            confusion_valid.batch_add(batch_val_y, preds)\n",
    "            if i==0:\n",
    "                valid_loss = cur_loss/(batch_size*num_gpus)\n",
    "            else:\n",
    "                valid_loss = valid_loss*i/(i+1) + cur_loss/(batch_size*num_gpus*(i+1))\n",
    "            val_queue.get_queue().task_done()\n",
    "            sys.stdout.write('\\rValidation. batch: %d/%d. loss: %f, acc.: %f'%(i+1,num_batches_valid,valid_loss,confusion_valid.accuracy()))\n",
    "            sys.stdout.flush()\n",
    "            sleep(1)\n",
    "            \n",
    "        best_val_loss = valid_loss\n",
    "        valid_acc_cur = confusion_valid.accuracy()\n",
    "        best_val_acc = valid_acc_cur\n",
    "        print('\\nInitial validation loss and accuracy are: %f / %f'%(best_val_loss, valid_acc_cur))                \n",
    "    \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            if (lr != lr_scheme[epoch]):\n",
    "                lr = lr_scheme[epoch]\n",
    "                print('using lr', lr)\n",
    "        \n",
    "            t0 = time.time()                \n",
    "                        \n",
    "            confusion_train = ConfusionMatrix(2)\n",
    "            train_queue.produce()\n",
    "            for i in range(num_batches_train):                \n",
    "                (batch_train_x, batch_train_y) = train_queue.get_queue().get()                \n",
    "                feed_dict_train = {\n",
    "                    x_pl: batch_train_x,\n",
    "                    y_pl: batch_train_y,\n",
    "                    is_training: True, \n",
    "                    learning_rate: lr\n",
    "                }\n",
    "                fetches_train = [train_op, losses, y]\n",
    "                res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "                cur_loss = np.sum(res[1])\n",
    "                preds = np.argmax(np.concatenate(res[2]), axis=-1)\n",
    "                confusion_train.batch_add(batch_train_y, preds)\n",
    "                if i==0:\n",
    "                    train_loss = cur_loss/(batch_size*num_gpus)\n",
    "                else:\n",
    "                    train_loss = train_loss*i/(i+1) + cur_loss/(batch_size*num_gpus*(i+1))                                \n",
    "                train_queue.get_queue().task_done()\n",
    "                sys.stdout.write('\\rTraining. batch: %d/%d, loss: %f, acc.: %f'%(i+1,num_batches_train,train_loss,confusion_train.accuracy()))\n",
    "                sys.stdout.flush()\n",
    "                sleep(1)\n",
    "                    \n",
    "            t1 = time.time()\n",
    "            epoch_time = t1 - t0\n",
    "        \n",
    "            sys.stdout.write(\"\\n\")\n",
    "        \n",
    "            confusion_valid = ConfusionMatrix(2)            \n",
    "            val_queue.produce()\n",
    "            for i in range(num_batches_valid):\n",
    "                (batch_val_x, batch_val_y) = val_queue.get_queue().get()                \n",
    "                feed_dict_eval = {\n",
    "                    x_pl: batch_val_x,\n",
    "                    y_pl: batch_val_y,\n",
    "                    is_training: False\n",
    "                }\n",
    "                fetches_eval = [y, losses]\n",
    "                # running the validation\n",
    "                res = sess.run(fetches=fetches_eval, feed_dict=feed_dict_eval)\n",
    "                # collecting and storing predictions\n",
    "                cur_loss = np.sum(res[1])\n",
    "                preds = np.argmax(np.concatenate(res[0]), axis=-1)             \n",
    "                confusion_valid.batch_add(batch_val_y, preds)\n",
    "                if i==0:\n",
    "                    valid_loss = cur_loss/(batch_size*num_gpus)\n",
    "                else:\n",
    "                    valid_loss = valid_loss*i/(i+1) + cur_loss/(batch_size*num_gpus*(i+1))\n",
    "                val_queue.get_queue().task_done()\n",
    "                sys.stdout.write('\\rValidation. batch: %d/%d, loss: %f, acc.: %f'%(i+1,num_batches_valid,valid_loss,confusion_valid.accuracy()))\n",
    "                sys.stdout.flush()\n",
    "                sleep(1)\n",
    "                            \n",
    "            sys.stdout.write(\"\\n\")\n",
    "            \n",
    "            train_acc_cur = confusion_train.accuracy()\n",
    "            valid_acc_cur = confusion_valid.accuracy()\n",
    "\n",
    "            train_acc += [train_acc_cur]\n",
    "            valid_acc += [valid_acc_cur]\n",
    "            print (\"Epoch %i: train loss %e, train acc. %f, valid loss %f, valid acc %f, epoch time %.2f s \" \\\n",
    "            % (epoch+1, train_loss, train_acc_cur, valid_loss, valid_acc_cur, epoch_time))\n",
    "        \n",
    "            if (best_val_loss<0):\n",
    "                best_val_loss = valid_loss\n",
    "            \n",
    "            if ((best_val_loss>=0) and (valid_loss<best_val_loss)):\n",
    "                print('val loss improved from %f to %f, saving model' % (best_val_loss, valid_loss))\n",
    "                best_val_loss = valid_loss\n",
    "                if (output_dir):\n",
    "                    filename = output_dir + 'best_model_loss'\n",
    "                    print('saving model to file:',filename)\n",
    "                    saver.save(sess, filename)\n",
    "                    \n",
    "            if (best_val_acc<valid_acc_cur):\n",
    "                print('val acc improved from %f to %f, saving model' % (best_val_acc, valid_acc_cur))\n",
    "                best_val_acc = valid_acc_cur\n",
    "                if (output_dir):\n",
    "                    filename = output_dir + 'best_model_acc'\n",
    "                    print('saving model to file:',filename)\n",
    "                    saver.save(sess, filename)\n",
    "                        \n",
    "            if ((epoch%10)==0):\n",
    "                saver.save(sess, output_dir+'/checkpoint-epoch-%.3d-loss-%.4f-acc-%.3f'%(epoch,valid_loss,valid_acc_cur))\n",
    "                \n",
    "            epoch += 1\n",
    "            \n",
    "            train_loss = 0.\n",
    "            valid_loss = 0.\n",
    "            \n",
    "    except KeyboardInterrupt:        \n",
    "        pass\n",
    "    \n",
    "    train_queue.get_queue().join()\n",
    "    val_queue.get_queue().join()\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "    epoch = np.arange(len(train_acc))\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_acc,'r', epoch, valid_acc,'b')\n",
    "    plt.legend(['Train Acc','Val Acc'])\n",
    "    plt.xlabel('Epochs'), plt.ylabel('Acc'), plt.ylim([0.75,1.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "num_epochs = 100\n",
    "prev_model = ''\n",
    "\n",
    "# for optimal performances, train 5 models with varying seed\n",
    "# when splitting set into training and validation.\n",
    "# final nodule predictions will be averaged over the 5 models.\n",
    "\n",
    "output_directory = 'model_1/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "train(train_data=train_data, \n",
    "    train_targets=train_targets,\n",
    "    validation_data=validation_data, \n",
    "    validation_targets=validation_targets,\n",
    "    num_gpus=num_gpus,\n",
    "    with_bn=True,\n",
    "    output_dir=output_directory,\n",
    "    prev_model=prev_model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
